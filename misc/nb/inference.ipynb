{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch.cuda.amp import autocast\n",
    "from contextlib import nullcontext\n",
    "from imwatermark import WatermarkEncoder\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/share/project/yfl/codebase/git/AltTools/Altdiffusion/src')\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from ldm.models.diffusion.dpm_solver import DPMSolverSampler\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "def load_model_from_config(config, ckpt, use_ema, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    \n",
    "    if use_ema:\n",
    "        sd = pl_sd[\"state_dict_ema\"]\n",
    "    else:\n",
    "        sd = pl_sd[\"state_dict\"]\n",
    "    # 模型是在这个地方初始化，初始化的\n",
    "    model = instantiate_from_config(config.model)\n",
    "\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "class OPT():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "opt = OPT()\n",
    "opt.prompt = \"一个中国小男孩\"\n",
    "opt.steps = 50\n",
    "opt.ddim_eta = 0.0\n",
    "opt.n_iter = 1\n",
    "opt.H=512\n",
    "opt.W=512\n",
    "opt.C = 4\n",
    "opt.f = 8\n",
    "opt.n_samples = 4\n",
    "opt.n_rows = 0\n",
    "opt.scale = 9.0\n",
    "opt.plms = False\n",
    "opt.dpm = False\n",
    "opt.fixed_code = False\n",
    "opt.precision = 'autocast'\n",
    "opt.use_ema = False\n",
    "\n",
    "opt.config = \"/share/project/yfl/codebase/git/AltTools/Altdiffusion/src/configs/v2-inference-alt.yaml\"\n",
    "# opt.ckpt = \"/share/project/yfl/codebase/stable_diffusion_2.0/stablediffusion/logs/train_2.1_small_lr/checkpoints/epoch=000000.ckpt\"\n",
    "# opt.ckpt = \"/share/project/yfl/codebase/stable_diffusion_2.0/ckpt/v2-1_512-ema-pruned.ckpt\"\n",
    "# opt.ckpt = \"/share/project/yfl/database/stable_diffusion_2.0/512-base-ema.ckpt\"\n",
    "# opt.ckpt = \"/share/project/yfl/codebase/stable_diffusion_2.0/stablediffusion/logs/train_2.1_train_use_penultimate/checkpoints/epoch=000000.ckpt\"\n",
    "# opt.ckpt = \"/share/project/yfl/database/stable_diffusion_2.0/512-base-ema.ckpt\"\n",
    "# opt.ckpt = \"/share/project/yfl/codebase/stable_diffusion_2.0/stablediffusion/logs/train_2.1_train_use_penultimate_large_lr/checkpoints/epoch=000002.ckpt\"\n",
    "# opt.ckpt = \"/share/project/yfl/codebase/stable_diffusion_2.0/stablediffusion/logs/train_2.1_train_use_penultimate_large_lr_1e06/checkpoints/epoch=000002.ckpt\"\n",
    "# opt.ckpt = \"/share/project/yfl/codebase/stable_diffusion_2.0/stablediffusion/logs/train_2.1_train_use_penultimate_large_lr_1e05/checkpoints/epoch=000001.ckpt\"\n",
    "# opt.ckpt = \"/share/project/yfl/codebase/stable_diffusion_2.0/stablediffusion/logs/test_lr/checkpoints/step=00050000.ckpt\"\n",
    "# opt.ckpt = \"/share/project/yfl/codebase/stable_diffusion_2.0/stablediffusion/logs/m18/checkpoints/epoch=000000.ckpt\"\n",
    "# opt.ckpt = \"/share/project/yfl/codebase/git/AltTools/Altdiffusion/ckpt/laion5plus_real/checkpoints/step=000450000.ckpt\"\n",
    "# opt.ckpt = \"/share/project/liuguang/alt_ckpts/step=000180000.ckpt\"\n",
    "# opt.ckpt = \"/share/project/yfl/codebase/git/AltTools/Altdiffusion/ckpt/laion5plus_256_kv/checkpoints/step=000030000.ckpt\"\n",
    "# opt.ckpt = '/share/project/yfl/codebase/git/AltTools/Altdiffusion/ckpt/ema_xformer_laion6plus_512_all_new_v2/checkpoints/step=000007500.ckpt'\n",
    "# opt.ckpt = '/share/project/yfl/codebase/git/AltTools/Altdiffusion/ckpt/laion5plus_256_kv/checkpoints/step=000030000.ckpt'\n",
    "# opt.ckpt = \"/share/project/yfl/codebase/stable_diffusion_2.0/stablediffusion/logs/test_lr/checkpoints/step=00037441.ckpt\"\n",
    "# opt.ckpt = '/share/project/yfl/codebase/git/AltTools/Altdiffusion/ckpt/xformer_laion5plus_512_kv_cfg/checkpoints/step=000015000.ckpt'\n",
    "opt.ckpt = '/share/project/yfl/codebase/git/AltTools/Altdiffusion/ckpt/ckpt_20230315/step=000330000.ckpt'\n",
    "# opt.ckpt = '/share/project/yfl/database/ckpt/aethetics_all_ema_cfg/step=000025000.ckpt'\n",
    "\n",
    "config = OmegaConf.load(f\"{opt.config}\")\n",
    "model = load_model_from_config(config, f\"{opt.ckpt}\", opt.use_ema)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "if opt.plms:\n",
    "    sampler = PLMSSampler(model)\n",
    "elif opt.dpm:\n",
    "    sampler = DPMSolverSampler(model)\n",
    "else:\n",
    "    sampler = DDIMSampler(model)\n",
    "\n",
    "\n",
    "def generate(prompt, negative_prompt, seed, opt):\n",
    "    seed_everything(seed)\n",
    "    batch_size = opt.n_samples\n",
    "    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
    "\n",
    "    start_code = None\n",
    "    if opt.fixed_code:\n",
    "        start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
    "\n",
    "    precision_scope = autocast if opt.precision == \"autocast\" else nullcontext\n",
    "    with torch.no_grad(), \\\n",
    "        precision_scope(True), \\\n",
    "        model.ema_scope():\n",
    "            all_samples = list()\n",
    "            prompts = [batch_size * [prompt]]\n",
    "            for prompts in tqdm(prompts, desc=\"data\"):\n",
    "                uc = None\n",
    "                if opt.scale != 1.0:\n",
    "                    uc = model.get_learned_conditioning(batch_size * [negative_prompt])\n",
    "                if isinstance(prompts, tuple):\n",
    "                    prompts = list(prompts)\n",
    "                c = model.get_learned_conditioning(prompts)\n",
    "                shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
    "                samples, _ = sampler.sample(S=opt.steps,\n",
    "                                                    conditioning=c,\n",
    "                                                    batch_size=opt.n_samples,\n",
    "                                                    shape=shape,\n",
    "                                                    verbose=False,\n",
    "                                                    unconditional_guidance_scale=opt.scale,\n",
    "                                                    unconditional_conditioning=uc,\n",
    "                                                    eta=opt.ddim_eta,\n",
    "                                                    x_T=start_code)\n",
    "\n",
    "                x_samples = model.decode_first_stage(samples)\n",
    "                x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                for x_sample in x_samples:\n",
    "                    x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                    img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "\n",
    "                all_samples.append(x_samples)\n",
    "\n",
    "            # additionally, save as grid\n",
    "            grid = torch.stack(all_samples, 0)\n",
    "            grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "            grid = make_grid(grid, nrow=n_rows)\n",
    "\n",
    "            # to image\n",
    "            grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "            grid = Image.fromarray(grid.astype(np.uint8))\n",
    "            grid.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"A Pikachu\"\n",
    "# prompt = \"一个穿着 EVA 插头套装的漂亮女孩的超写实绘画，超详细，动漫，作者 greg rutkowski，在 artstation 上流行\"\n",
    "# prompt = \"ジャケット、ビクトリア朝、コンセプト アート、詳細な顔、ファンタジー、顔のクローズ アップ、非常に詳細な、映画のような照明、グレッグ rutkowski によるデジタル アートの絵画で頑丈な 19 世紀の男の肖像\"\n",
    "# prompt = \"Hyper realistic painting of a beautiful girl in an EVA plugsuit, hyper detailed, anime, by greg rutkowski, trending on artstation\"\n",
    "# prompt = \"Pikachu commiting tax fraud, paperwork, exhausted, cute, really cute, cozy,by steve hanks, by lisa yuskavage, by serov valentin, by tarkovsky, 8 k render, detailed, cute cartoon style\"\n",
    "negative_prompt = \"nsfw, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\"\n",
    "# negative_prompt = ''\n",
    "prompt = \"a lively magical town inspired by victorian england and amsterdam, sunny weather, highly detailed, intricate, digital painting, trending on artstation, concept art, matte painting, art by greg rutkwowski, craig mullins, octane render, 8 k, unreal engine\"\n",
    "# negative_prompt = \"low quality\"\n",
    "# negative_prompt = \"low quality\"\n",
    "seed = 455561\n",
    "opt.W = 512\n",
    "opt.H = 768\n",
    "generate(prompt,negative_prompt,seed, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
